<!DOCTYPE html>
<html lang="zh-CN ">

<head>
    <meta charset="UTF-8" />
    <meta name="google" content="notranslate" />
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <title> HuLingOAO </title>

    
<link rel="stylesheet" href="/css/main.css">


    
        <script>
    MathJax = {
        tex: {
            tags: 'ams',
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    

</head>


<body>
    <div id="topbar">
    <div class="topbar-content">
        <ul class="topbar-menu">
            
            
                
                <li class="topbar-menu-item">
                    <a href="/" title="首页">首页</a>
                </li>
                
            
                
                <li class="topbar-menu-item">
                    <a href="/archives" title="归档">归档</a>
                </li>
                
            
                
                <li class="topbar-menu-item">
                    <a href="/categories" title="分类">分类</a>
                </li>
                
            
                
                <li class="topbar-menu-item">
                    <a href="/tags" title="标签">标签</a>
                </li>
                
            
                
                <li class="topbar-menu-item">
                    <a href="/about" title="关于我">关于我</a>
                </li>
                
            
        </ul>
        
        <div class="article-top-bar">
            <a class="go-back" href="javascript:history.back()">&lt; 返回</a>
            <a class="go-to-top" href="#top-title">回到顶部</a>
        </div>
        
    </div>
</div>
<main>
    <div class="left-sidebar">
        
            <p class="info-title">目录</p>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-text">基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81%E2%80%9D-%E4%B8%8E-%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D"><span class="toc-text">定义 “特征” 与 “标签”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E2%80%9D-%E5%85%B3%E7%B3%BB"><span class="toc-text">定义 “特征-标签” 关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98"><span class="toc-text">提出问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E9%97%AE%E9%A2%98"><span class="toc-text">使用数学语言描述问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E8%A1%A8%E8%BE%BE"><span class="toc-text">引入向量进行表达</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98"><span class="toc-text">进一步分析问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">引入数据集与训练集的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-text">引入最小二乘法转化问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">为什么使用最小二乘法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98"><span class="toc-text">解决问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-text">牛顿法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3%E6%B3%95"><span class="toc-text">解析解法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89"><span class="toc-text">符号定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86"><span class="toc-text">一些定理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-text">解析式推导</span></a></li></ol></li></ol></li></ol>
        
    </div>

    <article class="main-content">
        <h1 style="text-align: center;" id="top-title">Linear Regression - 线性回归</h1>
        <div class="inline-list">
        
            <p class="inline-list-title">分类</p>
            <ul class="inline-list-post-categories-list"><li class="inline-list-post-categories-list-item"><a class="inline-list-post-categories-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul>
            
        
            <p class="inline-list-title">标签</p>
            <ul class="inline-list-post-tags-list" itemprop="keywords"><li class="inline-list-post-tags-list-item"><a class="inline-list-post-tags-list-link" href="/tags/%E5%92%95%E5%92%95%E5%92%95ing/" rel="tag">咕咕咕ing</a></li><li class="inline-list-post-tags-list-item"><a class="inline-list-post-tags-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="inline-list-post-tags-list-item"><a class="inline-list-post-tags-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="inline-list-post-tags-list-item"><a class="inline-list-post-tags-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>
        
        </div>
        <blockquote>
<p>基于 Andrew NG 机器学习 公开课程 <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><br />
线性回归是一种 <strong>监督学习 (supervised leaning)</strong> 的模型<br />
简单解释其中的数学原理，方便回顾</p>
</blockquote>
<h2 id="%E5%9F%BA%E7%A1%80" id="基础">基础</h2>
<h3 id="%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81%E2%80%9D-%E4%B8%8E-%E2%80%9C%E6%A0%87%E7%AD%BE%E2%80%9D" id="定义-“特征”-与-“标签”">定义 “特征” 与 “标签”</h3>
<p>这里的定义是为了方便后面的表达</p>
<p>将监督学习中的一些概念 “重命名”</p>
<ul>
<li>特征，定义为输入空间（特征空间）</li>
<li>标签，定义为输出空间</li>
</ul>
<p>对 “特征” 与 “标签” 的理解</p>
<ol>
<li>
<p>从算法定义上</p>
<ul>
<li>特征可理解为输入</li>
<li>标签可理解为输出</li>
<li>算法的目的就是根据输入，给出输出</li>
</ul>
</li>
<li>
<p>从直观感觉上</p>
<ul>
<li>特征指代一个对象的特点</li>
<li>标签指代一个对象</li>
<li>算法的目的即根据特点，猜测对象</li>
</ul>
</li>
</ol>
<h3 id="%E5%AE%9A%E4%B9%89-%E2%80%9C%E7%89%B9%E5%BE%81-%E6%A0%87%E7%AD%BE%E2%80%9D-%E5%85%B3%E7%B3%BB" id="定义-“特征-标签”-关系">定义 “特征-标签” 关系</h3>
<p>实际生活中，我们知道了 $A, B$ 能够在某种程度上决定 $C$<br />
且 $A, B$ 各自对 $C$ 的影响有多大，我们并不清楚</p>
<p>也就是说，我们知道 “特征” 与 “标签” 之间存在某种关系，但无法准确地描述这个关系</p>
<p>此时我们可以对 $A, B, C$ 建立一个关系<br />
即：能由 $A, B$ 推出 $C$<br />
使用数学符号表示： $(A, B) \Rightarrow C$</p>
<p>若将 $A$ 和 $B$ 称做 $C$ 的特征<br />
并且将 $C$ 定义为标签<br />
将 $(A, B) \Rightarrow C$ 称为 “特征-标签” 关系<br />
则称数据 $(A, B, C)$ 存在 “特征-标签” 关系</p>
<h3 id="%E6%8F%90%E5%87%BA%E9%97%AE%E9%A2%98" id="提出问题">提出问题</h3>
<p>我们希望，给出一组存在 “特征-标签” 的关系的数据<br />
计算机能够根据这组数据，建立一个存在 “特征-标签” 关系的模型<br />
随后，给出 “特征” ，计算机能够根据模型，给出比较可靠的、推测的 “标签”</p>
<h3 id="%E4%BD%BF%E7%94%A8%E6%95%B0%E5%AD%A6%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%E9%97%AE%E9%A2%98" id="使用数学语言描述问题">使用数学语言描述问题</h3>
<p>接下来将问题抽象成为一个数学问题</p>
<p>设所有的特征的集合为 $(x_{1}, x_{2}, \cdots, x_{n})$，其中，$n$ 表示特征的数量<br />
$y$ 为特征对应的 “标签”</p>
<p>在比较简单的情况下<br />
假定 “特征-标签” 是线性关系<br />
并且这个线性关系为函数 $h$<br />
则称 $h$ 为 <strong>假设函数 (hypotheses)</strong></p>
<p>于是得到的函数</p>
<p>$$
\begin{equation}
h_{\theta}(x) = \theta_{0} + \theta_{1} \cdot x_{1} + \theta_{2} \cdot x_{2} + \cdots + \theta_{n} \cdot x_{n}
\end{equation}
$$</p>
<p>其中， $\theta_{0}, \theta_{1}, \cdots, \theta_{n}$ 表示 <strong>参数 (parameters)</strong>，或者说是 <strong>权值 (weights)</strong><br />
这是我们需要求得的值</p>
<h3 id="%E5%BC%95%E5%85%A5%E5%90%91%E9%87%8F%E8%BF%9B%E8%A1%8C%E8%A1%A8%E8%BE%BE" id="引入向量进行表达">引入向量进行表达</h3>
<p>若用向量表示 $\theta$ ：</p>
<p>$$
\vec{\theta} =
\begin{pmatrix}
\theta_{0}\\
\theta_{1}\\
\vdots\\
\theta_{n}
\end{pmatrix}
$$</p>
<p>并将特征集合重新定义为特征向量：</p>
<p>$$
\vec{x} =
\begin{pmatrix}
x_{0}\\
x_{1}\\
\vdots\\
x_{n}
\end{pmatrix}
$$</p>
<p>其中 $x_{0} = 1$<br />
<strong>为简化符号，记 $\vec{\theta}$ 为 $\theta$ ，$\vec{x}$ 为 $x$</strong><br />
则函数可表示为：</p>
<p>$$
\begin{equation}
h_{\theta}(x) = \sum_{i=0}^{n}{\theta_{i} x_{i}} = \theta^{T}x
\end{equation}
$$</p>
<p>此时我们的问题就是，求出一个 $\theta$ ，使得函数 $h$ 能够很好地描述 “特征-标签” 关系<br />
即建立一个合理的模型</p>
<h2 id="%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E9%97%AE%E9%A2%98" id="进一步分析问题">进一步分析问题</h2>
<h3 id="%E5%BC%95%E5%85%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9A%84%E6%A6%82%E5%BF%B5" id="引入数据集与训练集的概念">引入数据集与训练集的概念</h3>
<p>上面的函数仅表示单个数据中的 “特征与标签” 关系<br />
若存在多个数据，即 <strong>数据集 (dataset)</strong><br />
且所有的数据都存在 “特征-标签” 的关系<br />
即可以构建出一个 <strong>训练集 (training set)</strong><br />
我们可以使用 <strong>训练集</strong> 对算法进行 <strong>训练</strong> ，最终得出 $\theta$</p>
<p>定义下面的符号，方便之后的表示：<br />
设 <strong>训练集大小</strong> 为 $m$ ，令：<br />
$x_{i}^{(j)}$ 表示训练集中， <strong>第 $j$ 项数据</strong> 的 <strong>第 $i$ 个特征</strong><br />
$x^{(j)}$ 表示训练集中， <strong>第 $j$ 项数据</strong> 的特征向量<br />
同理可得 $\theta_{i}^{(j)}$ 与 $\theta^{(j)}$</p>
<h3 id="%E5%BC%95%E5%85%A5%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E8%BD%AC%E5%8C%96%E9%97%AE%E9%A2%98" id="引入最小二乘法转化问题">引入最小二乘法转化问题</h3>
<p>回顾我们的问题：<br />
我们希望得到一个函数 $h$ 能够很好地描述训练集中的每个数据中 “特征-标签” 的关系<br />
换句话说，给出 “特征” ，若 “特征” 存在于训练集中，那么
由 <strong>函数 $h$</strong> 得出的 “标签”，与训练集中的 “标签”，它们之间的误差最小<br />
根据 <strong>最小二乘法</strong> ，得：</p>
<p>$$
\begin{equation}
J(\theta) = \frac{1}{2} \sum_{i=1}^{m} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right)^{2}
\end{equation}
$$</p>
<p>函数 $J$ 也称为 <strong>cost function</strong></p>
<p>于是问题转化为：<br />
需要得到一个 $\theta$ ，使得 $J(\theta)$ 的值最小</p>
<h3 id="%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" id="为什么使用最小二乘法">为什么使用最小二乘法</h3>
<p>咕咕咕</p>
<h2 id="%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98" id="解决问题">解决问题</h2>
<p>接下来介绍解决这个问题的可行方法</p>
<h3 id="%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D" id="梯度下降">梯度下降</h3>
<p>为了得到 $\theta$ 使函数值 $J(\theta)$ 最小，可以采取 <strong>梯度下降 (gradient descent)</strong> 的方法：<br />
随机选取一个 $\theta$ 值，随后不断地更新 $\theta$ ，使 $J(\theta)$ 不断减小<br />
当 $J(\theta)$ 的值达到最小时，得到的 $\theta$ 即是问题的解</p>
<p>我们可以这样进行更新：
$$<br />
\begin{equation}
\theta_{j} := \theta_{j} - \alpha \frac {\partial}{\partial \theta_{j}} (\theta)
\label{eq4}
\end{equation}
$$</p>
<p>其中 $:=$ 表示赋值，将右边的值赋予左边，而 $=$ 是真值判断，表示等号两边的值或结果是相同的<br />
$\alpha$ 表示 <strong>学习率 (learning rate)</strong>，或者称 <strong>步长</strong>，是自定义的常数</p>
<p>对 $J(\theta)$ 进行偏导：
$$
\begin{equation}
\begin{split}
\frac{\partial}{\partial \theta_{j}} J(\theta)
&amp;= \frac{\partial}{\partial \theta_{j}} \frac {1}{2} (h_{\theta}(x) - y)^2 \\
&amp;= 2 \cdot \frac{1}{2}(h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_{j}} (h_{\theta}(x) - y) \\
&amp;= (h_{\theta}(x) - y) \cdot \frac{\partial}{\partial \theta_{j}} \left( \sum_{i=0}^{n} \theta_{i} x_{i} - y \right) \\
&amp;= (h_{\theta}(x) - y) x_{j}
\end{split}
\end{equation}
$$</p>
<p>代入 <strong>$\eqref{eq4}$</strong> ，得：</p>
<p>$$
\begin{equation}
\theta_{j} := \theta_{j} - \alpha (h_{\theta}(x) - y) x_{j}
\end{equation}
$$</p>
<p>这是单个数据的表示，引入上面介绍的训练集</p>
<p>$$
\begin{equation}
\theta_{j} := \theta_{j} - \alpha \sum_{i=1}^{m} \left(h_{\theta}(x^{(i)}) - y^{(i)}\right) x_{j}^{(i)}
\end{equation}
$$</p>
<p>对每一个在向量 $\theta$ 中的分量 $\theta_{j}$ 都进行上述运算<br />
最终我们会得到一个 $\theta$ ，这个值就是我们希望求得的值</p>
<p><em>注： 梯度下降算法的解析可参考另一篇文章</em></p>
<h3 id="%E7%89%9B%E9%A1%BF%E6%B3%95" id="牛顿法">牛顿法</h3>
<blockquote>
<p>尚未完成，咕咕咕ing</p>
</blockquote>
<p>因为极值点处的一阶导数为 0<br />
且 $J(\theta)$ 的函数最高次为二次，函数只存在一个极值点
所以，为了得到 $J(\theta)$ 的最小值，我们可以使用间接的方法<br />
即：寻找 $J(\theta)$ 导数的零点</p>
<p>牛顿法的步骤为：</p>
<blockquote>
<p>设函数为 $f(x)$</p>
<ol>
<li>随机在函数 $f(x)$ 上取一初始点 $P_{0}$</li>
<li>过点 $P_{0}$ 作函数 $f(x)$ 的切线 $L$</li>
<li>找到切线 $L$ 与 $x$ 轴的交点 $Q_{0}$</li>
<li>过交点 $Q_{0}$ 作 $x$ 轴的垂线，并与函数有一交点 $P_{1}$</li>
<li>将点 $P_{1}$ 作为初始点，回到步骤 2</li>
<li>当点 $P_{n}$ 与点 $Q_{n}$ 重合时，横坐标即零点</li>
</ol>
</blockquote>
<h3 id="%E8%A7%A3%E6%9E%90%E8%A7%A3%E6%B3%95" id="解析解法">解析解法</h3>
<p>实际上，上面的两种方法得到的是一个近似值<br />
但其精确度能够满足实际应用<br />
而解析解法能够直接计算出精确值</p>
<h4 id="%E7%AC%A6%E5%8F%B7%E5%AE%9A%E4%B9%89" id="符号定义">符号定义</h4>
<blockquote>
<p>这里的定义均来自 Andrew NG 课程讲义</p>
</blockquote>
<p><strong>定义矩阵函数的导函数</strong>
对于函数 $f$ : $\mathbb{R}^{m \times n} \mapsto \mathbb{R}$<br />
即 $m \times n$ 的矩阵映射至实数的函数关系<br />
定义函数 $f$ 对矩阵 $A$ 的导数为：</p>
<p>$$
\begin{equation}
\nabla_{A}f(A) =
\begin{bmatrix}
\frac{\partial{f}}{\partial A_{11}} &amp; \cdots &amp; \frac{\partial{f}}{\partial A_{1n}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial{f}}{\partial A_{m1}} &amp; \cdots &amp; \frac{\partial{f}}{\partial A_{mn}} \\
\end{bmatrix}
\end{equation}
$$</p>
<p><em>例子</em><br />
若矩阵
$
A =
\begin{bmatrix}
A_{11} &amp; A_{12} \\
A_{21} &amp; A_{22}
\end{bmatrix}
$ ，
函数 $f(A) = \frac{3}{2} A_{11} + 5A_{12}^{2} + A_{21}A_{22}$ ，
那么
$
\nabla_{A} f(A) =
\begin{bmatrix}
\frac{3}{2} &amp; 10A_{12} \\
A_{22} &amp; A_{21}
\end{bmatrix}
$</p>
<p><strong>定义 trace 运算</strong></p>
<p><code>trace</code> 运算简写为 <code>tr</code><br />
对于 $n \times n$ 的矩阵 $A$ ，定义</p>
<p>$$
\begin{equation}
\mathrm{tr}A = \sum_{i=1}^{n}A_{ii}
\end{equation}
$$</p>
<p>可以知道， <code>trace</code> 运算具有下列运算法则</p>
<p>$$
\begin{equation}
\label{algorithm1}
\mathrm{tr}ABC = \mathrm{tr}CAB = \mathrm{tr}BCA
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{algorithm2}
\mathrm{tr}(A+B) = \mathrm{tr}A + \mathrm{tr}B
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{algorithm3}
\mathrm{tr}aA = a\mathrm{tr}A
\end{equation}
$$
其中 $a$ 是一个实数</p>
<h4 id="%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86" id="一些定理">一些定理</h4>
<p>一些 <strong>trace 运算</strong> 与 <strong>矩阵导函数</strong> 相关的定理<br />
仅列出 <strong>解析式推导过程</strong> 需要用到的一些定理，不作证明</p>
<p>$$
\begin{equation}
\label{theorem1}
\nabla_{A} \mathrm{tr}AB = B^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem2}
\nabla_{A^{T}} f(A) = (\nabla_{A}f(A))^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem3}
\nabla_{A} \mathrm{tr} A B A^{T} C = C A B + C^{T} A B^{T}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\label{theorem4}
\nabla_{A} |A| = |A| ( A^{-1} )^{T}
\end{equation}
$$</p>
<h4 id="%E8%A7%A3%E6%9E%90%E5%BC%8F%E6%8E%A8%E5%AF%BC" id="解析式推导">解析式推导</h4>
<p>使用矩阵表达 “特征” 与 “标签”
定义矩阵 $X$ 和 $Y$</p>
<p>$$
\begin{equation}
X =
\begin{bmatrix}
( x^{(1)} )^{T} \\
( x^{(2)} )^{T} \\
\vdots \\
( x^{(m)} )^{T}
\end{bmatrix}
,
Y =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
\end{equation}
$$</p>
<blockquote>
<p>注意，此处 $x$ 表示向量 $\vec{x}$ ， $y$ 表示特征</p>
</blockquote>
<p>因为 $h_{\theta}(x^{(i)}) = \theta^{T} x^{(i)} = ( x^{(i)} )^{T} \theta$ ，故</p>
<p>$$
\begin{equation}
h_{\theta}(x)=
\begin{bmatrix}
( x^{(1)} )^{T} \theta \\
( x^{(2)} )^{T} \theta \\
\vdots \\
( x^{(m)} )^{T} \theta
\end{bmatrix}
= X \theta
\end{equation}
$$</p>
<p>又因为</p>
<p>$$
\begin{equation}
X \theta - Y =
\begin{bmatrix}
( x^{(1)} )^{T} \theta \\
( x^{(2)} )^{T} \theta \\
\vdots \\
( x^{(m)} )^{T} \theta
\end{bmatrix} -
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix} =
\begin{bmatrix}
h_{\theta}(x^{(1)}) - y^{(1)} \\
h_{\theta}(x^{(2)}) - y^{(2)} \\
\vdots \\
h_{\theta}(x^{(m)}) - y^{(m)}
\end{bmatrix}
\end{equation}
$$</p>
<p>且对于矩阵 $z$ ，有 $z^{T}z = \sum_{i} z_{i}^{2}$<br />
于是可得</p>
<p>$$
\begin{equation}
\begin{split}
\frac{1}{2} (X \theta - Y)^{T} (X \theta - Y)
&amp;= \frac{1}{2} \sum_{i = 1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} )^{2} \\
&amp;= J(\theta)
\end{split}
\end{equation}
$$</p>
<p>根据 <strong>$\eqref{theorem2}$</strong> 和 <strong>$\eqref{theorem3}$</strong> ，有</p>
<p>$$
\begin{equation}
\label{inference1}
\nabla_{A^{T}} \mathrm{tr} AB A^{T} C = B^{T} A^{T} C^{T} + B A^{T} C
\end{equation}
$$</p>
<p>于是</p>
<p>$$
\begin{equation}
\begin{split}
\nabla_{\theta} J(\theta)
&amp;= \nabla_{\theta} \frac{1}{2} ( X \theta - Y )^{T} (X \theta - Y) \\
&amp;= \frac{1}{2} \nabla_{\theta} ( \theta^{T} X^{T} X \theta - \theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\
&amp;= \frac{1}{2} \nabla_{\theta} \mathrm{tr} ( \theta^{T} X^{T} X \theta - \theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y ) \\
&amp;= \frac{1}{2} \nabla_{\theta} ( \mathrm{tr} \theta^{T} X^{T} X \theta - 2 \mathrm{tr} Y^{T} X \theta ) \\
&amp;= \frac{1}{2} ( X^{T} X \theta + X^{T} X \theta - 2 X^{T} Y ) \\
&amp;= X^{T}  X \theta - X^{T} Y
\end{split}
\end{equation}
$$</p>
<p>第三步将 $(\theta^{T} X^{T} X \theta - \theta^{T} X^{T} Y - Y^{T} X \theta + Y^{T} Y )$ 视作整体<br />
可知这是一个实数，可将其看作一个 $1 \times 1$ 的矩阵 $A = \begin{bmatrix}a\end{bmatrix}$
由 <code>trace</code> 定义可知 $\mathrm{tr} A = a$<br />
第四步使用了 <strong>$\mathrm{tr} A = \mathrm{tr} A^{T}$</strong><br />
第五步中使用了 <strong>$\eqref{theorem1}$</strong> 和 <strong>$\eqref{inference1}$</strong> ，视 $A^{T} = \theta$ , $B = B^{T} = X^{T} X$ , $C = I$</p>
<p>使 $J(\theta)$ 最小，则令 $\nabla_{\theta} J(\theta) = 0$<br />
所以有 $X^{T}  X \theta = X^{T} Y$</p>
<p>因此，得 $\theta = ( X^{T}X )^{-1} X^{T} Y$</p>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>课程链接 <a target="_blank" rel="noopener" href="https://see.stanford.edu/Course/CS229">https://see.stanford.edu/Course/CS229</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

    </article>

    <div class="right-sidebar">
        
            <p class="info-title">标签</p>



<ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E5%92%95%E5%92%95%E5%92%95ing/" rel="tag">咕咕咕ing</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag">监督学习</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>

        
        
            <p class="info-title">分类</p>



<ul class="post-categories-list"><li class="post-categories-list-item"><a class="post-categories-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul>

        
    </div>
</main>

</body>


    
<link rel="stylesheet" href="/css/article.css">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>


</html>